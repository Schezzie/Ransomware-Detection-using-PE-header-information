{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaC8nbkoeRr1",
        "outputId": "916a5015-e244-4659-ee69-f5be807714a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pefile in /usr/local/lib/python3.10/dist-packages (2023.2.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pefile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o01AP0qdtSUF",
        "outputId": "32cb85ac-5d07-407e-e6b6-f7970f7f3ddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pefile in /usr/local/lib/python3.10/dist-packages (2023.2.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pefile pandas scikit-learn scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51MBeEa3eIe3"
      },
      "outputs": [],
      "source": [
        "import pefile\n",
        "import mmap\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.io import arff\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDljnSd923md"
      },
      "outputs": [],
      "source": [
        "def extract_pe_header(file_path):\n",
        "    try:\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            mm = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)\n",
        "            pe = pefile.PE(data=mm, fast_load=True)\n",
        "            pe_header = mm.read(1024)\n",
        "            return pe_header\n",
        "    except pefile.PEFormatError as e:\n",
        "        print(f\"Error parsing file: {e}\")\n",
        "        return None\n",
        "\n",
        "def save_to_arff(pe_header_data, output_file, id):\n",
        "    with open(output_file, \"a\") as f:\n",
        "        f.write(f\"{id},0,\")\n",
        "        for i, byte in enumerate(pe_header_data):\n",
        "            f.write(f\"{byte},\")\n",
        "        f.write(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yo0a3XNx22NA"
      },
      "outputs": [],
      "source": [
        "# Folder path containing the executable files\n",
        "folder_path = r\"/content/drive/MyDrive/\"\n",
        "\n",
        "# Output file path for the consolidated .arff\n",
        "output_file = \"pe_header.arff\"\n",
        "\n",
        "# Write the arff header information if the file doesn't exist\n",
        "if not os.path.exists(output_file):\n",
        "    with open(output_file, \"w\") as f:\n",
        "        f.write(\"@relation pe_header\\n\")\n",
        "        f.write(\"\\n\")\n",
        "        f.write(\"@attribute ID numeric\\n\")\n",
        "        f.write(\"@attribute GR numeric\\n\")\n",
        "        for i in range(1024):\n",
        "            f.write(f\"@attribute {i} numeric\\n\")\n",
        "        f.write(\"\\n\")\n",
        "        f.write(\"@data\\n\")\n",
        "\n",
        "# ID counter starting from 13000\n",
        "id_counter = 13000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq1po5mL2vso",
        "outputId": "5a63956c-b39a-402b-cbcd-0e84032f7a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X train shape:  (1725, 1024)\n",
            "X train scaled shape:  (1725, 1024)\n",
            "Y train shape:  (1725,)\n"
          ]
        }
      ],
      "source": [
        "# Load the training dataset\n",
        "data, meta = arff.loadarff('/content/drive/MyDrive/dataset.arff')\n",
        "df = pd.DataFrame(data)\n",
        "df = df.drop(['ID', 'filename', 'family'], axis=1)\n",
        "X = df.drop(['GR'], axis=1)\n",
        "y = df['GR']\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features based on training data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Train the Random Forest classifier\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "# Create the directory for processed files if it doesn't exist\n",
        "processed_folder = r\"downloads_processed\"\n",
        "os.makedirs(processed_folder, exist_ok=True)\n",
        "print(\"X train shape: \",X_train.shape)\n",
        "print(\"X train scaled shape: \",X_train_scaled.shape)\n",
        "print(\"Y train shape: \",y_train.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8TPxuEHCsuq"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "-eVJgCwv3WHa",
        "outputId": "a276e096-81be-4c02-b46f-85be1d3e28a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ],
            "text/html": [
              "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "rf.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq2RWrf53sqo",
        "outputId": "8dcc6c01-31dd-431d-9a31-42ece4f18a4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.95      0.96       222\n",
            "         1.0       0.95      0.97      0.96       210\n",
            "\n",
            "    accuracy                           0.96       432\n",
            "   macro avg       0.96      0.96      0.96       432\n",
            "weighted avg       0.96      0.96      0.96       432\n",
            "\n",
            "Confusion Matrix:\n",
            "[[211  11]\n",
            " [  6 204]]\n",
            "Accuracy: 0.9606481481481481\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Scale the test features based on training data\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf.predict(X_test_scaled)\n",
        "\n",
        "# Print the classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAaCHBsTRa8y",
        "outputId": "2220bb2f-6ca9-4343-8c9d-d51872e555b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.948837209302326\n",
            "Recall: 0.971428571428571\n",
            "F1-score: 0.960000000000000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# Assuming rf is your trained Random Forest model and X_test, y_test are your test features and labels\n",
        "\n",
        "# Scale the test features based on training data\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf.predict(X_test_scaled)\n",
        "\n",
        "# Calculate precision, recall, F1-score, and support\n",
        "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "\n",
        "# Print the precision, recall, and F1-score with many decimal points\n",
        "print(f'Precision: {precision:.15f}')\n",
        "print(f'Recall: {recall:.15f}')\n",
        "print(f'F1-score: {fscore:.15f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlvEpFVA4ZwN"
      },
      "source": [
        "# RNN implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_F2W-VCe4Orr",
        "outputId": "38728db9-c21b-43d7-c204-947a6d771c97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "54/54 [==============================] - 2s 6ms/step - loss: 0.3137 - accuracy: 0.8730\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.1189 - accuracy: 0.9612\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0793 - accuracy: 0.9786\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.0515 - accuracy: 0.9849\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0347 - accuracy: 0.9930\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0268 - accuracy: 0.9954\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0179 - accuracy: 0.9965\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0155 - accuracy: 0.9971\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0114 - accuracy: 0.9977\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 0s 5ms/step - loss: 0.0068 - accuracy: 1.0000\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 0.1564 - accuracy: 0.9491\n",
            "Test Accuracy: 0.9490740895271301\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming X_train_scaled, y_train are already prepared from previous steps\n",
        "\n",
        "# Reshape X_train_scaled to have 3 dimensions (samples, time steps, features)\n",
        "X_train_reshaped = np.reshape(X_train_scaled, (X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
        "\n",
        "# Build the RNN model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.SimpleRNN(128, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Assuming X_test_scaled, y_test are already prepared from previous steps\n",
        "\n",
        "# Reshape X_test_scaled to have 3 dimensions (samples, time steps, features)\n",
        "X_test_reshaped = np.reshape(X_test_scaled, (X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq54sxKCAtGl",
        "outputId": "47b33000-96dd-4159-ea5f-acdc71481bc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 0s 4ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.94      0.95       222\n",
            "         1.0       0.94      0.96      0.95       210\n",
            "\n",
            "    accuracy                           0.95       432\n",
            "   macro avg       0.95      0.95      0.95       432\n",
            "weighted avg       0.95      0.95      0.95       432\n",
            "\n",
            "Confusion Matrix:\n",
            "[[209  13]\n",
            " [  9 201]]\n",
            "Accuracy: 0.9490740740740741\n",
            "Precision: 0.9392523364485982\n",
            "Recall: 0.9571428571428572\n",
            "F1-score: 0.9481132075471699\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_prob = model.predict(X_test_reshaped)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Flatten predictions and true labels for sklearn metrics\n",
        "y_pred_flat = y_pred.flatten()\n",
        "y_test_flat = y_test.to_numpy()\n",
        "\n",
        "# Calculate additional metrics\n",
        "accuracy = accuracy_score(y_test_flat, y_pred_flat)\n",
        "precision = precision_score(y_test_flat, y_pred_flat)\n",
        "recall = recall_score(y_test_flat, y_pred_flat)\n",
        "f1 = f1_score(y_test_flat, y_pred_flat)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_flat, y_pred_flat))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_flat, y_pred_flat))\n",
        "\n",
        "# Print additional metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acDlTE6E4mc9"
      },
      "source": [
        "# Ensemble model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoDcB0q7_WkX",
        "outputId": "0674f19d-e473-40ee-bcde-dffb3273f296"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "54/54 [==============================] - 4s 25ms/step - loss: 0.3094 - accuracy: 0.8771 - val_loss: 0.1888 - val_accuracy: 0.9375\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 1s 14ms/step - loss: 0.1192 - accuracy: 0.9606 - val_loss: 0.1360 - val_accuracy: 0.9468\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 1s 15ms/step - loss: 0.0631 - accuracy: 0.9826 - val_loss: 0.1200 - val_accuracy: 0.9537\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 1s 13ms/step - loss: 0.0346 - accuracy: 0.9948 - val_loss: 0.1168 - val_accuracy: 0.9583\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 0.0236 - accuracy: 0.9965 - val_loss: 0.1182 - val_accuracy: 0.9514\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 1s 12ms/step - loss: 0.0184 - accuracy: 0.9971 - val_loss: 0.1262 - val_accuracy: 0.9514\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 1s 22ms/step - loss: 0.0119 - accuracy: 0.9983 - val_loss: 0.1279 - val_accuracy: 0.9537\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 1s 21ms/step - loss: 0.0087 - accuracy: 0.9988 - val_loss: 0.1255 - val_accuracy: 0.9560\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 1s 20ms/step - loss: 0.0070 - accuracy: 0.9983 - val_loss: 0.1267 - val_accuracy: 0.9583\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 1s 19ms/step - loss: 0.0054 - accuracy: 0.9988 - val_loss: 0.1243 - val_accuracy: 0.9560\n",
            "54/54 [==============================] - 1s 4ms/step\n",
            "14/14 [==============================] - 0s 4ms/step\n",
            "Ensemble Model Accuracy: 0.9583333333333334\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.94      0.95       222\n",
            "         1.0       0.94      0.96      0.95       210\n",
            "\n",
            "    accuracy                           0.95       432\n",
            "   macro avg       0.95      0.95      0.95       432\n",
            "weighted avg       0.95      0.95      0.95       432\n",
            "\n"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape the data for LSTM input\n",
        "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build and train the LSTM model\n",
        "lstm_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Extract features using the LSTM model\n",
        "lstm_features = lstm_model.predict(X_train)\n",
        "\n",
        "# Train a Random Forest classifier on the extracted features\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(lstm_features, y_train)\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "lstm_test_features = lstm_model.predict(X_test)\n",
        "y_pred_rf = rf_classifier.predict(lstm_test_features)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"Ensemble Model Accuracy:\", accuracy_rf)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmaLTWU5EDTN",
        "outputId": "0d70ad75-1ec8-4135-d37c-4afd55d90178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.97      0.95      0.96       222\n",
            "         1.0       0.94      0.97      0.96       210\n",
            "\n",
            "    accuracy                           0.96       432\n",
            "   macro avg       0.96      0.96      0.96       432\n",
            "weighted avg       0.96      0.96      0.96       432\n",
            "\n",
            "Confusion Matrix:\n",
            "[[210  12]\n",
            " [  6 204]]\n",
            "Accuracy: 0.9583333333333334\n",
            "Precision: 0.9444444444444444\n",
            "Recall: 0.9714285714285714\n",
            "F1-score: 0.9577464788732395\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_prob = rf_classifier.predict(lstm_test_features)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Flatten predictions and true labels for sklearn metrics\n",
        "y_test_flat = y_test.to_numpy()\n",
        "\n",
        "# Calculate additional metrics\n",
        "accuracy = accuracy_score(y_test_flat, y_pred)\n",
        "precision = precision_score(y_test_flat, y_pred)\n",
        "recall = recall_score(y_test_flat, y_pred)\n",
        "f1 = f1_score(y_test_flat, y_pred)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_flat, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_flat, y_pred))\n",
        "\n",
        "# Print additional metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kfTyy0B44OO"
      },
      "source": [
        "# CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b2GMeBf4sLC",
        "outputId": "3864171f-efc4-4213-ac61-dc14367b8fe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "54/54 [==============================] - 6s 60ms/step - loss: 0.4435 - accuracy: 0.8209\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 3s 63ms/step - loss: 0.1911 - accuracy: 0.9322\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 5s 85ms/step - loss: 0.1278 - accuracy: 0.9507\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 2s 36ms/step - loss: 0.0961 - accuracy: 0.9658\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 2s 31ms/step - loss: 0.0626 - accuracy: 0.9786\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 2s 32ms/step - loss: 0.0466 - accuracy: 0.9878\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 2s 33ms/step - loss: 0.0327 - accuracy: 0.9901\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 2s 33ms/step - loss: 0.0237 - accuracy: 0.9954\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 2s 37ms/step - loss: 0.0217 - accuracy: 0.9959\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 3s 54ms/step - loss: 0.0182 - accuracy: 0.9942\n",
            "14/14 [==============================] - 0s 8ms/step - loss: 0.1928 - accuracy: 0.9514\n",
            "Test Accuracy: 0.9513888955116272\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Reshape X_train_scaled and X_test_scaled to have 3 dimensions (samples, timesteps, features)\n",
        "X_train_reshaped = np.reshape(X_train_scaled, (X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
        "X_test_reshaped = np.reshape(X_test_scaled, (X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
        "\n",
        "# Build the CNN model for 1D data\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv1D(32, 3, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)),\n",
        "    tf.keras.layers.MaxPooling1D(2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_qqzFRTBEQD",
        "outputId": "f3d383bb-f696-43f3-a35f-8d37f5c9b818"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 0s 8ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.94      0.97      0.95       222\n",
            "         1.0       0.97      0.93      0.95       210\n",
            "\n",
            "    accuracy                           0.95       432\n",
            "   macro avg       0.95      0.95      0.95       432\n",
            "weighted avg       0.95      0.95      0.95       432\n",
            "\n",
            "Confusion Matrix:\n",
            "[[215   7]\n",
            " [ 14 196]]\n",
            "Accuracy: 0.9513888888888888\n",
            "Precision: 0.9655172413793104\n",
            "Recall: 0.9333333333333333\n",
            "F1-score: 0.9491525423728815\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_prob = model.predict(X_test_reshaped)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Flatten predictions and true labels for sklearn metrics\n",
        "y_test_flat = y_test.to_numpy()\n",
        "\n",
        "# Calculate additional metrics\n",
        "accuracy = accuracy_score(y_test_flat, y_pred)\n",
        "precision = precision_score(y_test_flat, y_pred)\n",
        "recall = recall_score(y_test_flat, y_pred)\n",
        "f1 = f1_score(y_test_flat, y_pred)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_flat, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_flat, y_pred))\n",
        "\n",
        "# Print additional metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9PIBhkw83yS"
      },
      "source": [
        "# Dataset 2:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uKL7Gqx8YgG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = '/content/drive/MyDrive/pe_section_headers.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Preprocess the data\n",
        "X = df[['size_of_data', 'virtual_address', 'entropy', 'virtual_size']]\n",
        "y = df['malware']\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape the data for LSTM input\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO1Ud1Ss89C_"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JRCNccz8_jl",
        "outputId": "b295ad4e-516f-4820-e342-bef0819a5011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9672017553990068\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.42      0.51       354\n",
            "           1       0.98      0.99      0.98      8305\n",
            "\n",
            "    accuracy                           0.97      8659\n",
            "   macro avg       0.82      0.70      0.75      8659\n",
            "weighted avg       0.96      0.97      0.96      8659\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 147  207]\n",
            " [  77 8228]]\n"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the Random Forest model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Print classification report and confusion matrix\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU-rcIYSSLda",
        "outputId": "2d5d7f7c-641d-4fe0-b96e-7b4056554788"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.9754593953764078\n",
            "Recall: 0.990728476821192\n",
            "F1-score: 0.9830346475507766\n"
          ]
        }
      ],
      "source": [
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the metrics\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1-score: {f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV38sPiIRi0P",
        "outputId": "7a1b80b0-e458-4014-e66a-227762578670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.959117681025523\n",
            "Recall: 1.000000000000000\n",
            "F1-score: 0.979132280122613\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# Assuming rf is your trained Random Forest model and X_test, y_test are your test features and labels\n",
        "\n",
        "# Scale the test features based on training data\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate precision, recall, F1-score, and support\n",
        "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "\n",
        "# Print the precision, recall, and F1-score with many decimal points\n",
        "print(f'Precision: {precision:.15f}')\n",
        "print(f'Recall: {recall:.15f}')\n",
        "print(f'F1-score: {fscore:.15f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7Uz1iL89enA"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8fsd75M9d9p",
        "outputId": "0861f7b7-6c45-447c-906a-8850ec5cfc77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1083/1083 [==============================] - 7s 4ms/step - loss: 0.1896 - accuracy: 0.9561 - val_loss: 0.1653 - val_accuracy: 0.9592\n",
            "Epoch 2/10\n",
            "1083/1083 [==============================] - 3s 3ms/step - loss: 0.1608 - accuracy: 0.9605 - val_loss: 0.1614 - val_accuracy: 0.9593\n",
            "Epoch 3/10\n",
            "1083/1083 [==============================] - 3s 3ms/step - loss: 0.1576 - accuracy: 0.9607 - val_loss: 0.1574 - val_accuracy: 0.9595\n",
            "Epoch 4/10\n",
            "1083/1083 [==============================] - 5s 5ms/step - loss: 0.1541 - accuracy: 0.9604 - val_loss: 0.1536 - val_accuracy: 0.9593\n",
            "Epoch 5/10\n",
            "1083/1083 [==============================] - 3s 3ms/step - loss: 0.1516 - accuracy: 0.9605 - val_loss: 0.1520 - val_accuracy: 0.9593\n",
            "Epoch 6/10\n",
            "1083/1083 [==============================] - 3s 3ms/step - loss: 0.1498 - accuracy: 0.9605 - val_loss: 0.1517 - val_accuracy: 0.9593\n",
            "Epoch 7/10\n",
            "1083/1083 [==============================] - 4s 3ms/step - loss: 0.1489 - accuracy: 0.9604 - val_loss: 0.1497 - val_accuracy: 0.9593\n",
            "Epoch 8/10\n",
            "1083/1083 [==============================] - 4s 4ms/step - loss: 0.1487 - accuracy: 0.9605 - val_loss: 0.1484 - val_accuracy: 0.9593\n",
            "Epoch 9/10\n",
            "1083/1083 [==============================] - 3s 3ms/step - loss: 0.1480 - accuracy: 0.9605 - val_loss: 0.1515 - val_accuracy: 0.9592\n",
            "Epoch 10/10\n",
            "1083/1083 [==============================] - 3s 3ms/step - loss: 0.1476 - accuracy: 0.9605 - val_loss: 0.1511 - val_accuracy: 0.9593\n",
            "271/271 [==============================] - 0s 2ms/step - loss: 0.1511 - accuracy: 0.9593\n",
            "Loss: 0.15113796293735504, Accuracy: 0.9593486785888672\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Preprocess the data\n",
        "X = df[['size_of_data', 'virtual_address', 'entropy', 'virtual_size']]\n",
        "y = df['malware']\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape the data for Simple RNN input\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# Build the RNN model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.SimpleRNN(128, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, validation_data=(X_test_reshaped, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
        "print(f'Loss: {loss}, Accuracy: {accuracy}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eCIV2QbKBbYx",
        "outputId": "71f4b36a-0b80-40a6-eb15-c7c210281d04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "271/271 [==============================] - 1s 2ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.01      0.02       354\n",
            "           1       0.96      1.00      0.98      8305\n",
            "\n",
            "    accuracy                           0.96      8659\n",
            "   macro avg       0.81      0.51      0.50      8659\n",
            "weighted avg       0.95      0.96      0.94      8659\n",
            "\n",
            "Confusion Matrix:\n",
            "[[   4  350]\n",
            " [   2 8303]]\n",
            "Accuracy: 0.9593486545790507\n",
            "Precision: 0.9595516006009477\n",
            "Recall: 0.9997591812161348\n",
            "F1-score: 0.9792428352400047\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_prob = model.predict(X_test_reshaped)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Flatten predictions and true labels for sklearn metrics\n",
        "y_pred_flat = y_pred.flatten()\n",
        "y_test_flat = y_test.to_numpy()\n",
        "\n",
        "# Calculate additional metrics\n",
        "accuracy = accuracy_score(y_test_flat, y_pred_flat)\n",
        "precision = precision_score(y_test_flat, y_pred_flat)\n",
        "recall = recall_score(y_test_flat, y_pred_flat)\n",
        "f1 = f1_score(y_test_flat, y_pred_flat)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_flat, y_pred_flat))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_flat, y_pred_flat))\n",
        "\n",
        "# Print additional metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUdIu8AF9-jH"
      },
      "source": [
        "Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud4CMhFB9_cb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = '/content/drive/MyDrive/pe_section_headers.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Preprocess the data\n",
        "X = df[['size_of_data', 'virtual_address', 'entropy', 'virtual_size']]\n",
        "y = df['malware']\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape the data for LSTM input\n",
        "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build and train the LSTM model\n",
        "lstm_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Extract features using the LSTM model\n",
        "lstm_features = lstm_model.predict(X_train)\n",
        "\n",
        "# Train a Random Forest classifier on the extracted features\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(lstm_features, y_train)\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "lstm_test_features = lstm_model.predict(X_test)\n",
        "y_pred_rf = rf_classifier.predict(lstm_test_features)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"Ensemble Model Accuracy:\", accuracy_rf)\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MriC6-oHBr1M"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_prob = rf_classifier.predict(lstm_test_features)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Flatten predictions and true labels for sklearn metrics\n",
        "y_test_flat = y_test.to_numpy()\n",
        "\n",
        "# Calculate additional metrics\n",
        "accuracy = accuracy_score(y_test_flat, y_pred)\n",
        "precision = precision_score(y_test_flat, y_pred)\n",
        "recall = recall_score(y_test_flat, y_pred)\n",
        "f1 = f1_score(y_test_flat, y_pred)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_flat, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_flat, y_pred))\n",
        "\n",
        "# Print additional metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC4MgTHm-P9s"
      },
      "source": [
        "CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiYOafR2-PU4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = '/content/drive/MyDrive/pe_section_headers.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Preprocess the data\n",
        "X = df[['size_of_data', 'virtual_address', 'entropy', 'virtual_size']]\n",
        "y = df['malware']\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape the data for CNN input\n",
        "X_reshaped = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the CNN model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    tf.keras.layers.MaxPooling1D(2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Loss: {loss}, Accuracy: {accuracy}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKcU2HzfCDQN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Flatten predictions and true labels for sklearn metrics\n",
        "y_pred_flat = y_pred.flatten()\n",
        "y_test_flat = y_test.to_numpy()\n",
        "\n",
        "# Calculate additional metrics\n",
        "accuracy = accuracy_score(y_test_flat, y_pred_flat)\n",
        "precision = precision_score(y_test_flat, y_pred_flat)\n",
        "recall = recall_score(y_test_flat, y_pred_flat)\n",
        "f1 = f1_score(y_test_flat, y_pred_flat)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_flat, y_pred_flat))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_flat, y_pred_flat))\n",
        "\n",
        "# Print additional metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0AK8cUSDY2t"
      },
      "outputs": [],
      "source": [
        "folder_path = r\"/content/drive/MyDrive/MALWARE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-0akr4Y2sBn"
      },
      "outputs": [],
      "source": [
        "# Infinite loop to continuously check for new files\n",
        "while True:\n",
        "    # Iterate over all files in the folder\n",
        "    file_list = os.listdir(folder_path)\n",
        "\n",
        "    # Check if there are any files in the directory\n",
        "    if len(file_list) == 0:\n",
        "        break\n",
        "\n",
        "    for file_name in file_list:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Check if the file is an .exe file and hasn't been processed before\n",
        "        if os.path.isfile(file_path) and file_name.lower().endswith((\".exe\",)):\n",
        "            try:\n",
        "                # Extract PE header data\n",
        "                pe_header_data = extract_pe_header(file_path)\n",
        "\n",
        "                # Save the data to the consolidated .arff file with ID and GR\n",
        "                save_to_arff(pe_header_data, output_file, id_counter)\n",
        "\n",
        "                # Increment ID counter\n",
        "                id_counter += 1\n",
        "\n",
        "                # Copy the processed file to the processed folder\n",
        "                processed_file_path = os.path.join(processed_folder, file_name)\n",
        "                shutil.copy2(file_path, processed_file_path)\n",
        "                os.chmod(file_path, 0o777)\n",
        "\n",
        "                # Remove the original file (optional)\n",
        "                os.remove(file_path)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "    # Pause the loop execution for a specified duration (e.g., 5 seconds)\n",
        "    time.sleep(5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pie2F0kIJoRu"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "dHSehSRn2ctL",
        "outputId": "3be84813-d0c5-4ab7-c2dc-71d511f872be"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "StopIteration",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-5258f2220e45>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the new dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadarff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pe_header.arff'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Replace empty strings with a default value (e.g., '0' or np.nan)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/io/arff/_arffread.py\u001b[0m in \u001b[0;36mloadarff\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0mofile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_loadarff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mofile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# only close what we opened\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/io/arff/_arffread.py\u001b[0m in \u001b[0;36m_loadarff\u001b[0;34m(ofile)\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;31m# Parse the header file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mrel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Error while parsing header, error was: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/io/arff/_arffread.py\u001b[0m in \u001b[0;36mread_header\u001b[0;34m(ofile)\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrelation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStopIteration\u001b[0m: "
          ]
        }
      ],
      "source": [
        "    # Load the new dataset\n",
        "    new_data, new_meta = arff.loadarff('pe_header.arff')\n",
        "    new_df = pd.DataFrame(new_data)\n",
        "\n",
        "    # Replace empty strings with a default value (e.g., '0' or np.nan)\n",
        "    new_df.replace('', np.nan, inplace=True)  # Replace empty strings with np.nan\n",
        "\n",
        "    # Convert attribute columns to float type\n",
        "    for column in new_df.columns:\n",
        "        if new_df[column].dtype == object:  # Check if the column contains strings\n",
        "            new_df[column] = pd.to_numeric(new_df[column], errors='coerce')\n",
        "\n",
        "    new_df.dropna(inplace=True)\n",
        "\n",
        "    # Proceed if there are samples available in the new dataset\n",
        "    if new_df.shape[0] > 0:\n",
        "        # Drop unnecessary columns\n",
        "        new_df = new_df.drop(['ID', 'GR'], axis=1)\n",
        "\n",
        "        # Perform label encoding if applicable\n",
        "        if 'family' in new_df.columns:\n",
        "            label_encoder = LabelEncoder()\n",
        "            new_df['family'] = label_encoder.fit_transform(new_df['family'])\n",
        "\n",
        "        # Scale the features of the new dataset using the trained scaler\n",
        "        new_df_scaled = scaler.transform(new_df)\n",
        "\n",
        "        # Predict the \"GR\" column for the new dataset\n",
        "        new_predictions = rf.predict(new_df_scaled)\n",
        "\n",
        "        # Print predicted GR values\n",
        "        print(\"Predicted GR:\", new_predictions)\n",
        "\n",
        "        # Quarantine files with predicted GR value 1\n",
        "        quarantine_folder = r\"quarantine\"\n",
        "        os.makedirs(quarantine_folder, exist_ok=True)\n",
        "\n",
        "        for file_name, prediction in zip(os.listdir(processed_folder), new_predictions):\n",
        "            if file_name.lower().endswith((\".exe\",)) and prediction == 1:\n",
        "                file_path = os.path.join(processed_folder, file_name)\n",
        "                quarantined_file_path = os.path.join(quarantine_folder, file_name)\n",
        "\n",
        "                try:\n",
        "                    shutil.move(file_path, quarantined_file_path)\n",
        "                    print(f\"File {file_name} quarantined.\")\n",
        "                    os.remove('pe_header.arff')\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error moving file {file_name} to quarantine: {e}\")\n",
        "\n",
        "    else:\n",
        "        print(\"No new samples found in the dataset.\")\n",
        "    time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6p7elEZeZvI"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKTk0EBDePcF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}